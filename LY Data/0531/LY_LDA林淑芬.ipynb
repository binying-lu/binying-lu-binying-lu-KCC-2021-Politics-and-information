{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/embedding_character/token_list.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9b18512010e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# 載入模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;31m# pos = POS(\"./data\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# ner = NER(\"./data\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/ckiptagger/api.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, disable_cuda)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_token_to_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_embedding_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"embedding_character\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdisable_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/ckiptagger/api.py\u001b[0m in \u001b[0;36m_load_embedding\u001b[0;34m(embedding_dir)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_load_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0mtoken_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"token_list.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0mtoken_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0mvector_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vector_list.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/embedding_character/token_list.npy'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on May 31 2021\n",
    "@author: Te-Yu,Lu\n",
    "\"\"\"\n",
    "\n",
    "from ckiptagger import data_utils, construct_dictionary, WS, POS, NER\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 1000)    #設定最大能顯示1000rows\n",
    "pd.set_option(\"display.max_columns\", 1000)\n",
    "\n",
    "# import keras\n",
    "\n",
    "# 載入 ckiptagger 自訂詞典\n",
    "User_Dict = {}\n",
    "with open(\"KccLy2Dict2021.txt\",\"r\", encoding='utf-8-sig') as UDicts:\n",
    "    for udic in UDicts:\n",
    "        udWord = udic.strip().split(\" \")\n",
    "        if len(udWord) > 1:\n",
    "            User_Dict[udWord[0]] = udWord[1]\n",
    "        else:\n",
    "            User_Dict[udWord[0]] = 100    # 未給定權重者一律賦予預設值 150                \n",
    "dictionary = construct_dictionary(User_Dict)\n",
    "\n",
    "# 載入模型\n",
    "ws = WS(\"../data\")\n",
    "# pos = POS(\"./data\")\n",
    "# ner = NER(\"./data\")\n",
    "\n",
    "# 載入 StopWord\n",
    "stopword_list = []\n",
    "with open(\"../dict/KccStopWord2020.txt\",\"r\", encoding='utf-8-sig') as stopwords:\n",
    "    for stopword in stopwords:\n",
    "        stopword_list.append(stopword.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = pd.read_csv('委員逐字稿.csv',encoding='utf-8') #匯資料\n",
    "lin = word[word.Name == \"林淑芬\"]\n",
    "lin.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_lin = lin[['Speech']] #提出會用到的欄位\n",
    "title_lin.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_list= list(title_lin['Speech'])\n",
    "\n",
    "lin_str =(','.join('%s' %id for id in lin_list)) \n",
    "#print(lin_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 開始斷詞\n",
    "# sentence_segmentation = True,  => To consider delimiters\n",
    "# segment_delimiter_set = {\",\", \"。\", \":\", \"?\", \"!\", \";\"}), # This is the defualt set of delimiters\n",
    "# recommend_dictionary = dictionary1, # words in this dictionary are encouraged\n",
    "# coerce_dictionary = dictionary2, # words in this dictionary are forced\n",
    "strCut = ws([lin_str], sentence_segmentation=True,\n",
    "                 segment_delimiter_set = {'\"',\"\\r\\n\",\"，\",\"「\",\"」\",\",\",\"？\",'\\n', \"。\", \":\", \"?\", \"!\", \";\", \"、\"},\n",
    "                 coerce_dictionary = dictionary)\n",
    "\n",
    "\n",
    "print(\"------------CKIPTagger Word Segment ws(str,....) + 刪除標點及停用字 ---------------\")\n",
    "stayed_lst= []\n",
    "for word in strCut[0]:\n",
    "     # 刪除停用字\n",
    "    if word not in stopword_list:\n",
    "        stayed_lst.append(word)\n",
    "\n",
    "print(\" / \".join(stayed_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Input: \"KCC Data/CkipNewsCisTest-KccDict2020.txt\"\n",
    "Output: \"KCC Data/NewsCisTest-KccDict2020-LDA-TopicNum10.txt\"\n",
    "#CkipNewsCisTest-KccDict2020.txt\n",
    "\n",
    "@author: Johnson\n",
    "\"\"\"\n",
    "# When numpy version = 1.19.2 => gensim version should downgrade to 3.8.3\n",
    "from gensim import corpora, models, utils\n",
    "from gensim.models import LdaModel, CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "stayed_lst_new = [d.split() for d in stayed_lst]\n",
    "dict = corpora.Dictionary(numpy.asarray(stayed_lst_new))\n",
    "print(\"dict = \", dict)\n",
    "print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat mapping matrix of [word_id, word_frequency] for each word segment\n",
    "corpus = [dict.doc2bow(text) for text in stayed_lst_new]   \n",
    "print(\"corpus[:1] = \", corpus[:1])   \n",
    "print(\"------------------------------\")\n",
    "\n",
    "print(\"print lists of [word:frequency]\")\n",
    "[[print((dict[id], freq)) for id,freq in cp] for cp in corpus[:5]]\n",
    "print(\"======================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = models.ldamodel.LdaModel(corpus=corpus, id2word= dict, num_topics=10, passes=20, alpha='auto', chunksize=50, per_word_topics=True)\n",
    "# print(ldamodel.print_topics())   列出最重要的前若干个主题\n",
    "\n",
    "output = open(\"2_NewsCisTest-KccDict2020-LDA-TopicNum10.txt\", 'w', encoding='utf-8-sig')\n",
    "lstTM = lda_model.print_topics(num_topics=10, num_words=20)\n",
    "for tupleTM in lstTM:\n",
    "        str1 = \"\".join(str(x) for x in tupleTM)\n",
    "        #print(\"type(str1) = \", type(str1))\n",
    "        str1 += \"\\n\"\n",
    "        print(\"strTM = \", str1)\n",
    "        output.write(str1)\n",
    "output.close()\n",
    " \n",
    "print(\"------------------------------\")\n",
    "print(\"Perplexity = \", lda_model.log_perplexity(corpus))\n",
    "\n",
    "print(\"------------------------------\")\n",
    "cm = CoherenceModel(model=lda_model, texts=list(stayed_lst_new), dictionary=dict, coherence= 'c_v' )\n",
    "coher_lda = cm.get_coherence()\n",
    "print(\"\\n Coherence Score: \", coher_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
